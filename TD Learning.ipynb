{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA TAXI\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "Q = defaultdict(lambda : 0)\n",
    "n = defaultdict(lambda : 1) \n",
    "\n",
    "actionspace = range(env.action_space.n)\n",
    "greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "episode_scores = []\n",
    "for _ in range(100000):\n",
    "    state = env.reset()\n",
    "    current_score = 0.\n",
    "    for t in range(1000):\n",
    "        if epsilon > random.random() :\n",
    "            action = env.action_space.sample()\n",
    "        else :\n",
    "            action = greedy_action(state)\n",
    "\n",
    "        # SARSA\n",
    "        if t > 0 :\n",
    "            Q[(prev_state,prev_action)] = Q[(prev_state,prev_action)] + 1./n[(prev_state,prev_action)] * ( reward + gamma * Q[(state, action)] - Q[(prev_state,prev_action)] )\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        current_score += reward\n",
    "\n",
    "        if done :\n",
    "            Q[(prev_state,prev_action)] = Q[(prev_state,prev_action)] + 1./n[(state,action)] * ( reward - Q[(prev_state,prev_action)] )\n",
    "            break\n",
    "\n",
    "        prev_state, state, prev_action = state, next_state, action\n",
    "\n",
    "    episode_scores.append(current_score)\n",
    "\n",
    "\n",
    "plt.plot(episode_scores)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA FOR OUR for Cliff Walking Environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "# Returns updated Q-value for the most last experience\n",
    "def update_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None):\n",
    "    current = Q[state][action]\n",
    "    Qsa_next = Q[next_state][next_action] if next_state is not None else 0    \n",
    "    target = reward + (gamma * Qsa_next)\n",
    "    new_value = current + (alpha * (target - current))\n",
    "    return new_value\n",
    "\n",
    "\n",
    "# Action based on epsilon\n",
    "def epsilon_greedy(Q, state, nA, eps):\n",
    "    if random.random() > eps: # select greedy action with probability epsilon\n",
    "        return np.argmax(Q[state])\n",
    "    else:                     # otherwise, select an action randomly\n",
    "        return random.choice(np.arange(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main SARSA Algorithm for Cliff Walking Environment\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma=1.0, plot_every=100):\n",
    "    no_actions = env.action_space.n \n",
    "    Q = defaultdict(lambda: np.zeros(no_actions))\n",
    "    \n",
    "    # monitor performance\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    avg_scores = deque(maxlen=num_episodes)\n",
    "    \n",
    "    for episode in range(1, num_episodes+1):\n",
    "        if episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(episode, num_episodes), end=\"\")   \n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        eps = 1.0 / episode                              \n",
    "        action = epsilon_greedy(Q, state, no_action, eps)  \n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            score += reward                          \n",
    "            if not done:\n",
    "                next_action = epsilon_greedy(Q, next_state, nA, eps) \n",
    "                Q[state][action] = update_Q_sarsa(alpha, gamma, Q, \\\n",
    "                                                  state, action, reward, next_state, next_action)\n",
    "                \n",
    "                state = next_state  \n",
    "                action = next_action \n",
    "            if done:\n",
    "                Q[state][action] = update_Q_sarsa(alpha, gamma, Q, \\\n",
    "                                                  state, action, reward)\n",
    "                tmp_scores.append(score) \n",
    "                break\n",
    "        if (episode % plot_every == 0):\n",
    "            avg_scores.append(np.mean(tmp_scores))\n",
    "\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(avg_scores))    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_sarsa = sarsa(env, 5000, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
